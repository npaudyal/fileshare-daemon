# Performance Analysis - Fileshare App

## Current Issues (587MB file taking 5+ minutes)

### 1. Sequential Network Writes Despite "Parallel" Sending
**Evidence from logs**:
- Chunk write times show sequential pattern with 1-2 second gaps
- Chunk 0 â†’ Chunk 10: 818ms gap
- Chunk 110 â†’ Chunk 120: 1.2 second gap
- This indicates network layer bottleneck

### 2. Message Channel Bottleneck
All chunks go through `message_tx` channel which appears to process sequentially:
```rust
// Current flow:
ParallelChunkSender â†’ message_tx â†’ PeerManager â†’ Connection Write
```

### 3. TCP Write Blocking
The `ðŸ“¤ WRITE chunk X` logs show each chunk write takes significant time, suggesting:
- No write buffering at TCP level
- Possible TCP congestion
- Each 1MB chunk waits for full ACK before next write

### 4. Misleading Speed Calculation
- Reports 31 MB/s but actual throughput is ~2 MB/s
- Speed calculated on chunks queued, not chunks actually sent

## Root Cause Analysis

The parallel sending is ineffective because:
1. **Single TCP connection** - All chunks serialize through one socket
2. **No TCP tuning** - Default TCP buffer sizes too small for 1MB chunks
3. **Synchronous writes** - Each chunk write blocks until complete
4. **Message channel overhead** - Extra serialization/routing layer

## Recommended Solutions

### 1. Multiple TCP Connections
```rust
// Use 4-8 parallel TCP connections
struct ParallelConnection {
    connections: Vec<TcpStream>,
    current_idx: AtomicUsize,
}
```

### 2. TCP Socket Tuning
```rust
// Increase TCP buffer sizes
socket.set_send_buffer_size(4 * 1024 * 1024)?; // 4MB
socket.set_recv_buffer_size(4 * 1024 * 1024)?;
socket.set_nodelay(false)?; // Enable Nagle for better batching
```

### 3. Async Write Pipeline
```rust
// Use tokio::io::AsyncWriteExt with buffering
let mut writer = BufWriter::with_capacity(4 * 1024 * 1024, socket);
writer.write_all(&chunk_data).await?;
// Don't flush after each chunk - let buffer fill
```

### 4. Remove Message Channel for Chunks
```rust
// Direct socket write for chunks
impl PeerConnection {
    async fn write_chunk_direct(&mut self, chunk: &[u8]) -> Result<()> {
        self.writer.write_all(chunk).await?;
        Ok(())
    }
}
```

### 5. Proper Chunk Size
- Current 1MB chunks are too large for efficient pipelining
- Recommend 256KB chunks for better parallelism
- Allows more chunks in flight simultaneously

### 6. Connection Pooling
```rust
// Pool of connections per peer
struct PeerConnectionPool {
    connections: Vec<TcpStream>,
    round_robin: AtomicUsize,
}

impl PeerConnectionPool {
    async fn send_chunk(&self, chunk: TransferChunk) -> Result<()> {
        let idx = self.round_robin.fetch_add(1, Ordering::Relaxed) % self.connections.len();
        let conn = &self.connections[idx];
        // Send on selected connection
    }
}
```

## Expected Performance After Fixes

With proper implementation:
- 4 parallel connections Ã— 10 MB/s each = 40 MB/s total
- 587MB file / 40 MB/s = ~15 seconds (vs current 5+ minutes)

## Quick Wins

1. **Increase TCP buffers** - Immediate 2-3x improvement
2. **Reduce chunk size to 256KB** - Better pipelining
3. **Remove flush() after each chunk** - Let OS batch writes
4. **Use BufWriter with 4MB buffer** - Reduce syscalls

## Conclusion

The current "parallel" implementation is bottlenecked by:
- Single TCP connection
- Synchronous chunk writes  
- Small TCP buffers
- Message channel overhead

The fix requires true parallel connections, not just parallel chunk queuing.